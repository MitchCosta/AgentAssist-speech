# -*- coding: utf-8 -*-
"""cisco-scraper-small.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10uIHWTsZ5qMQ0DrSL_i85S27ISSASu67
"""



# Commented out IPython magic to ensure Python compatibility.
import requests
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from random import random
from time import sleep
from pandas.core import apply
import re
from sklearn.metrics.pairwise import cosine_similarity

# %matplotlib inline



def get_sub_category(url):

  cisco_response = requests.get(url)
  cisco_soup = BeautifulSoup(cisco_response.text, 'html.parser')

  page_title = cisco_soup.title.string
  links_list = []

  for div in cisco_soup.find_all("div", {"class":"tab-pane active"}):
    for link in div.select("a"):
      if link['href'].find('feed/cat'):
        #if link.has_attr('title'):
          #print(link['title'])
        links_list.append(link['href'])

  return page_title, links_list

def get_questions_links(my_url:str):

  cisco_response = requests.get(my_url)
  cisco_soup = BeautifulSoup(cisco_response.text, 'html.parser')
    
  page_title = cisco_soup.title.string

  links_list = []

  for div in cisco_soup.find_all("div", {"class":"tab-pane active"}):
    for link in div.select("a"):
      if link.has_attr('title'):
        #print(link['href'])
        #print(link['title'])
        links_list.append(link['href'])

  return page_title, links_list

def get_answers(my_url:str):

  # Input -> page url of question and answer
  # Returns -> Page title, List with answer paragraphs

  cisco_response = requests.get(my_url)
  cisco_soup = BeautifulSoup(cisco_response.text, 'html.parser')

  answer_text = []
  page_title = cisco_soup.title.string

  for div in cisco_soup.find_all("article", {"class":"answer"}):

    for paragraph in div.select("p"):
      answer_text.append(paragraph.getText(strip=True))

    for paragraph in div.select("li"):
      answer_text.append(paragraph.getText(strip=True))

      for inside in paragraph.select('a'):
        if inside.has_attr('href'):
          answer_text.pop()


  return page_title, answer_text



# Cisco's homepage
cisco_url = 'https://meeting-infohub.cisco.com/faq/'

# Use requests to retrieve data from a given URL
cisco_response = requests.get(cisco_url)

# Parse the whole HTML page using BeautifulSoup
cisco_soup = BeautifulSoup(cisco_response.text, 'html.parser')

# Title of the parsed page
cisco_soup.title.string



# Find all links in home page
links = [link.get('href') for link in cisco_soup.find_all('a')]
links

page_links = links[:6]  # remove 'Most popular FAQs links'
page_links

base_address = page_links.pop(0)

base_address = base_address[:-5]   # remove 'com/faq/'



print(base_address)
page_links

last_link = page_links.pop()

last_link



cisco_faq_data = pd.DataFrame(columns=['sub_category_title', 'questions_title', 'answer_title', 'answer_paragraphs', 'answer_link'])

# Cisco
# Search the first 4 categories since they have a similar structure
# The last category (Cisco Meeting Server Web App) has no questions groups

numero = 0

for link in page_links:

  category_link = base_address + link

  sub_category_title, sub_category_links = get_sub_category(category_link)
  sub_category_links.pop(0)
  sub_category_links = [base_address + s for s in sub_category_links]

  for questions_link in sub_category_links:
    questions_title, questions_links = get_questions_links(questions_link)
    questions_links = [base_address + s for s in questions_links]

    for answer_link in questions_links:
      answer_title, answer_paragraphs = get_answers(answer_link)

      numero += 1
      sleeping = 1 + random()

      print(numero, len(answer_paragraphs), sleeping, link, sub_category_title, questions_title, answer_title, answer_paragraphs, answer_link)
      cisco_faq_data = cisco_faq_data.append({'sub_category_title': sub_category_title,
                                              'questions_title': questions_title,
                                              'answer_title': answer_title,
                                              'answer_paragraphs': answer_paragraphs,
                                              'answer_link': answer_link}, ignore_index=True)
      
      sleep(sleeping)
      #sleep(5)



# Cisco
# Search the last category (Cisco Meeting Server Web App) has no questions groups

category_link = base_address + last_link

questions_title, questions_links = get_questions_links(category_link)
questions_links = [base_address + s for s in questions_links]

for answer_link in questions_links:
  answer_title, answer_paragraphs = get_answers(answer_link)

  numero += 1
  sleeping = 1 + random()

  print(numero, len(answer_paragraphs), sleeping, link, sub_category_title, questions_title, answer_title, answer_paragraphs, answer_link)
  cisco_faq_data = cisco_faq_data.append({'sub_category_title': questions_title,
                                          'questions_title': questions_title,
                                          'answer_title': answer_title,
                                          'answer_paragraphs': answer_paragraphs,
                                          'answer_link': answer_link}, ignore_index=True)
      
  sleep(sleeping)
  #sleep(5)

cisco_faq_data.tail(3)

cisco_faq_data.to_csv('cisco_faq.csv', index=False)







"""# Data cleaning"""



cisco_data = pd.read_csv('cisco_faq.csv')

cisco_data.shape

cisco_data.head(3)



cisco_data['sub_category_title'] = cisco_data['sub_category_title'].apply(lambda x: x.replace('Frequently Asked Questions - ', ''))

cisco_data.sub_category_title.unique()

cisco_data['questions_title'] = cisco_data['questions_title'].apply(lambda x: x.replace('Frequently Asked Questions - ', ''))

cisco_data.questions_title.unique()

cisco_data['answer_title'] = cisco_data['answer_title'].apply(lambda x: x.replace('Frequently Asked Questions - ', ''))

cisco_data.head(3)

"""## Save cleaned data to file"""

cisco_data.to_csv('cisco_faq_cleaned.csv', index=False)

